\documentclass[journal,10pt,onecolumn,draftclsnofoot,]{IEEEtran}

%\usepackage[retainorgcmds]{IEEEtrantools}
%\usepackage{bibentry}
\usepackage{xcolor,soul,framed} %,caption

\colorlet{shadecolor}{yellow}
% \usepackage{color,soul}
\usepackage[pdftex]{graphicx}
\graphicspath{{../pdf/}{../jpeg/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}

\usepackage[cmex10]{amsmath}
%Mathabx do not work on ScribTex => Removed
%\usepackage{mathabx}
\usepackage{array}
\usepackage{mdwmath}
\usepackage{mdwtab}
\usepackage{eqparbox}
\usepackage{url}


% ------------------------------------------------------------------------------

% Definitions of languages: ----------------------------------------------------
\usepackage{listings}
\lstdefinestyle{cStyle}{
  basicstyle=\scriptsize,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  keepspaces=true,
  numbersep=5pt,
  showspaces=false,
  gobble=4,
  tabsize=4,
  showstringspaces=false,
  showtabs=false,
}
\renewcommand*{\lstlistingname}{Code}

% ------------------------------------------------------------------------------


% \hyphenation{op-tical net-works semi-conduc-tor}

%\bstctlcite{IEEE:BSTcontrol}

%=== TITLE & AUTHORS ===========================================================
\begin{document}
\bstctlcite{Reinforcement Learning}
  \title{Reinforcement Learning}
  \author{Carlos~Matheus~Barros~da~Silva,~
  \IEEEmembership{Computer Engineering Bachelor Student of ITA}
  \\Prof. Marcos~Ricardo~Omena~de~Albuquerque~Máximo}

% The paper headers ------------------------------------------------------------
\markboth{INSTITUTO TECNOLÓGICO DE AERONÁUTICA, May~2019
}{Reinforcement Learning}

% ==============================================================================
\maketitle



% === ABSTRACT =================================================================
% ==============================================================================
\begin{abstract}

This paper evaluates the core concepts of the behind the Reinforcement Learning
theory on the environment of the Markov Decision Process (MDP) and Dynamic
programming.

It was observed how policy iteration and value itaration works and how they are
affected by the probability of correctly executing the chosen action factor
($\alpha$) and the discount factor($\gamma$).

It was observed that for a deterministic world ($\gamma = 1$ and $\alpha = 1$),
the learning is sensibly slower, which means that was required much more
iterations in order to the value converge when compared to a little decrease
on $\gamma$ and $\alpha$ ($\gamma = 0.98$ and $\alpha = 0.8$).

% === KEYWORDS =================================================================
% ==============================================================================
\begin{IEEEkeywords}
    Reinforcement Learning, Markov Decision Process, MDP, Dynamic Programming,  policy, states
\end{IEEEkeywords}
\end{abstract}

\IEEEpeerreviewmaketitle
% ==============================================================================
% ==============================================================================
% ==============================================================================


% === I. INTRODUCTION ==========================================================
% ==============================================================================
\section{Introduction}

\IEEEPARstart{R}{e}inforcement learning (RL) is an area of machine learning
concerned with how software agents ought to take actions in an environment so
as to maximize some notion of cumulative reward. Reinforcement learning is one
of three basic machine learning paradigms, alongside supervised learning
and unsupervised learning.

It differs from supervised learning in that labelled input/output pairs need
not be presented, and sub-optimal actions need not be explicitly corrected.
Instead the focus is finding a balance between exploration (of uncharted
territory) and exploitation (of current knowledge).

The environment is typically formulated as a Markov decision process (MDP),
as many reinforcement learning algorithms for this context utilize dynamic
programming techniques. The main difference between the classical dynamic
programming methods and reinforcement learning algorithms is that the latter
do not assume knowledge of an exact mathematical model of the MDP and they
target large MDPs where exact methods become infeasible.

% ==============================================================================
\section{Dynamic Programming Implementation}

The implementation was based on the file \textit{dynamic programming}.
The essence of the implementation is shown from Code \ref{code:policy_iteration} to Code \ref{code:changed_val}.

\lstinputlisting[
    language=python,
    caption={Code of function \textit{policy iteration}, executes policy iteration for a grid world.},
    label={code:policy_iteration},
    style=cStyle,
    firstline=217,
    lastline=229
]{./../code/dynamic_programming.py}

\lstinputlisting[
    language=python,
    caption={Code of function \textit{value iteration}, executes value iteration for a grid world.},
    label={code:value_iteration},
    style=cStyle,
    firstline=152,
    lastline=181
]{./../code/dynamic_programming.py}

\lstinputlisting[
    language=python,
    caption={Code of function \textit{policy evaluation}, executes policy evaluation fora a policy executed on a grid world. },
    label={code:policy_evaluation},
    style=cStyle,
    firstline=131,
    lastline=133
]{./../code/dynamic_programming.py}

\lstinputlisting[
    language=python,
    caption={Code of function \textit{evaluate}, will evaluate for the \textit{policy evaluation} function. },
    label={code:evaluate},
    style=cStyle,
    firstline=67,
    lastline=94
]{./../code/dynamic_programming.py}

\lstinputlisting[
    language=python,
    caption={Code of function \textit{changed val}, will check whether the value changed some of its elements values or not. },
    label={code:changed_val},
    style=cStyle,
    firstline=106,
    lastline=110
]{./../code/dynamic_programming.py}

\section{Dynamic Programming Analysis}

\subsection{Dynamic Programming Analysis With $\gamma = 1$ and $\alpha = 1$}

For this case, the output can be seen on the Code \ref{output1}.

\lstinputlisting[
    language=python,
    caption={Output for \textit{test dynamic programming} for $\gamma = 1$ and $\alpha = 1$ case. },
    label={output1},
    style=cStyle,
]{./../code/output1.txt}

\subsection{Dynamic Programming Analysis With $\gamma = 0.98$ and $\alpha = 0.8$}

For this case, the output can be seen on the Code \ref{output2}.

\lstinputlisting[
    language=python,
    caption={Output for \textit{test dynamic programming} for $\gamma = 0.98$ and $\alpha = 0.8$ case. },
    label={output2},
    style=cStyle,
]{./../code/output2.txt}

\section {Conclusion}

It is clear, therefore, that this experiment exemplifies that \textit{value iteration} algorithm allows in a given world define a value grid that leads to an optimal policy by getting a greedy solution. It also shows that this can be achieved by iterate in \textit{policy itaration}, by vary between evaluating a policy and getting a greedy approach, which leads to an optimal policy much quicker (with fewer iterations) when compared to \textit{value iteration}.

This paper also shown how policy iteration and value itaration works and how they are affected by the probability of correctly executing the chosen action factor ($\alpha$) and the discount factor($\gamma$).

It was observed that for a deterministic world ($\gamma = 1$ and $\alpha = 1$),
the learning is sensibly slower, which means that was required much more
iterations in order to the value converge when compared to a little decrease
on $\gamma$ and $\alpha$ ($\gamma = 0.98$ and $\alpha = 0.8$), which is as expected for this problem, since this increases the exploration and allows the algorithm find a better solution quicker.

\vfill
\end{document}
