\documentclass[journal]{IEEEtran}

%\usepackage[retainorgcmds]{IEEEtrantools}
%\usepackage{bibentry}
\usepackage{xcolor,soul,framed} %,caption

\colorlet{shadecolor}{yellow}
% \usepackage{color,soul}
\usepackage[pdftex]{graphicx}
\graphicspath{{../pdf/}{../jpeg/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}

\usepackage[cmex10]{amsmath}
%Mathabx do not work on ScribTex => Removed
%\usepackage{mathabx}
\usepackage{array}
\usepackage{mdwmath}
\usepackage{mdwtab}
\usepackage{eqparbox}
\usepackage{url}


% ----------------------------------------------

% Definitions of languages: ------------
\usepackage{listings}
\lstdefinestyle{cStyle}{
  basicstyle=\scriptsize,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  keepspaces=true,
  numbersep=5pt,
  showspaces=false,
  gobble=4,
  tabsize=4,
  showstringspaces=false,
  showtabs=false,
}
\renewcommand*{\lstlistingname}{Code}

% ----------------------------------------------




% \hyphenation{op-tical net-works semi-conduc-tor}

%\bstctlcite{IEEE:BSTcontrol}


%=== TITLE & AUTHORS ====================================================================
\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}
    \title{Neural Networks}
  \author{Carlos~Matheus~Barros~da~Silva,~\IEEEmembership{Computer Engineering Bachelor Student of ITA}\\Prof. Marcos~Ricardo~Omena~de~Albuquerque~Máximo}

% The paper headers
\markboth{INSTITUTO TECNOLÓGICO DE AERONÁUTICA, May~2019
}{Neural Networks}

% ====================================================================
\maketitle



% === ABSTRACT ==============================================================
% ============================================================================
\begin{abstract}

This paper evaluates a shallow Neural Network by test it in different scenarios with two simple tests and a color segmentation test.

It was observed that the Neural Network worked fine for those purposes, and in some case, the result was really good, in the greater than function test case, for example.

For the color segmentation, the Neural Network also provided a concise well-segmented result.

% === KEYWORDS ===============================================================
% ============================================================================
\begin{IEEEkeywords}
    Simple Evolution Strategy, SES, Covariance Matrix Adaptation Evolution Strategy, CMA-ES, optimization
\end{IEEEkeywords}
\end{abstract}

\IEEEpeerreviewmaketitle

% ====================================================================
% ====================================================================
% ====================================================================


% === I. INTRODUCTION ========================================================
% =============================================================================
\section{Introduction}

\IEEEPARstart{N}{e}ural networks (NN) are computing systems vaguely inspired by the biological neural networks and astrocytes that constitute animal brains. The neural network itself is not an algorithm, but rather a framework for many different machine learning algorithms to work together and process complex data inputs. Such systems "learn" to perform tasks by considering examples, generally without being programmed with any task-specific rules. For example, an image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as "cat" or "no cat" and using the results to identify cats in other images. They do this without any prior knowledge about cats, for example, that they have fur, tails, whiskers and cat-like faces. Instead, they automatically generate identifying characteristics from the learning material that they process.

An NN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it.

In common NN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called 'edges'. Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.

The original goal of the NN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board, and video games and medical diagnosis.

% ==========================================================================
\section{Neural Network Implementation}

The implementation was based on the file \textit{imitatino learning}. The essence of the implementation is shown in the Code \ref{code:imitation}.

\lstinputlisting[
    language=python,
    caption={Code of \textit{imitatino learning}},
    label={code:imitation},
    style=cStyle,
    firstline=53,
    lastline=84
]{./../code/imitation_learning.py}

\section{Neural Network Analysis}

\subsection{Keras' Neural Network Analysis without regularization}

In this analysis was used two test functions: \textit{sum greater than} and \textit{xor}. This initial analysis is not using regularization.

The Keras' Neural Network performed regular on \textit{sum greater than} function. On the graphs represented by the images from Image \ref{img:greater_cost_no_reg} to Image \ref{img:greater_classification_no_reg}. It is possible to verify that the result is overfitted on the Image \ref{img:greater_classification_no_reg} and the convergence is not so fast on the Image \ref{img:greater_cost_no_reg}.

\begin{figure}
  \begin{center}
  \includegraphics[width=2.8in]{./../code/sgz_result/convergence_sgz_l0_0.png}
  \caption{Convergence of cost function on greater than function test case, when it is not using regularization.}
  \label{img:greater_cost_no_reg}
  \end{center}
\end{figure}

\begin{figure}
  \begin{center}
  \includegraphics[width=2.8in]{./../code/sgz_result/dataset_sgz_l0_0.png}
  \caption{Dataset of greater than function.}
  \label{img:greater_data_set}
  \end{center}
\end{figure}

\begin{figure}
    \begin{center}
    \includegraphics[width=2.8in]{./../code/sgz_result/nn_classification_sgz_l0_0.png}
    \caption{Neural Network Classification on greater than function, when it is not using regularization.}
    \label{img:greater_classification_no_reg}
    \end{center}
\end{figure}

The Neural Network performed regular on \textit{xor} function. On the graphs represented by the images from Image \ref{img:xor_cost_no_reg} to Image \ref{img:xor_classification_no_reg}. It is also possible to see that in this case, it also heaped some overfit causing some distortion and leading to some mistakes on the data set on the graph represented by the Image \ref{img:xor_classification_no_reg}.

\begin{figure}
  \begin{center}
  \includegraphics[width=2.8in]{./../code/xor_result/convergence_xor_l0_0.png}
  \caption{Convergence of cost function on xor function test case, when it is not using regularization.}
  \label{img:xor_cost_no_reg}
  \end{center}
\end{figure}

\begin{figure}
  \begin{center}
  \includegraphics[width=2.8in]{./../code/xor_result/dataset_xor_l0_0.png}
  \caption{Dataset of xor function.}
  \label{img:xor_data_set}
  \end{center}
\end{figure}

\begin{figure}
    \begin{center}
    \includegraphics[width=2.8in]{./../code/xor_result/nn_classification_xor_l0_0.png}
    \caption{Neural Network Classification on xor function, when it is not using regularization.}
    \label{img:xor_classification_no_reg}
    \end{center}
\end{figure}

\subsection{Keras' Neural Network Analysis without regularization}

In this analysis was used the same two test functions: \textit{sum greater than} and \textit{xor}. But now using regularization $\lambda_{l_2} = 0.002$.

The Keras' Neural Network performed well on \textit{sum greater than} function. On the graphs represented by the images from Image \ref{img:greater_cost_reg} to Image \ref{img:greater_classification_reg}. It is possible to verify that the result now is much less overfitted on the Image \ref{img:greater_classification_reg}, it is much softer, and the convergence is now faster on the Image \ref{img:greater_cost_reg}.

\begin{figure}
  \begin{center}
  \includegraphics[width=2.8in]{./../code/sgz_result/convergence_sgz_l0_002.png}
  \caption{Convergence of cost function on greater than function test case, when it is using regularization.}
  \label{img:greater_cost_reg}
  \end{center}
\end{figure}

\begin{figure}
    \begin{center}
    \includegraphics[width=2.8in]{./../code/sgz_result/nn_classification_sgz_l0_002.png}
    \caption{Neural Network Classification on greater than function, when it is using regularization.}
    \label{img:greater_classification_reg}
    \end{center}
\end{figure}

The Neural Network performed well on \textit{xor} function. On the graphs represented by the images from Image \ref{img:xor_cost_reg} to Image \ref{img:xor_classification_reg}. It is also possible to see that in this case, it also heaped much less overfit leading to a much softer image on the graph represented by the Image \ref{img:xor_classification_reg}.

\begin{figure}
  \begin{center}
  \includegraphics[width=2.8in]{./../code/xor_result/convergence_xor_l0_002.png}
  \caption{Convergence of cost function on xor function test case, when it is using regularization.}
  \label{img:xor_cost_reg}
  \end{center}
\end{figure}

\begin{figure}
    \begin{center}
    \includegraphics[width=2.8in]{./../code/xor_result/nn_classification_xor_l0_002.png}
    \caption{Neural Network Classification on xor function, when it is using regularization.}
    \label{img:xor_classification_reg}
    \end{center}
\end{figure}

\subsection{Keras' Neural Network Analysis in Imitation Learning}

In order to do the Imitation Learning, it was used the Code \ref{code:imitation} implementation.

It was made using Keras, not using regularization, and using mean squared error.

The result of this Neural Network on the robot moviment can be seen on the graphs from Image \ref{} to Image \ref{}.

% \begin{figure}
%   \begin{center}
%   \includegraphics[width=2.8in]{./../code/result/original_image.png}
%   \caption{Original image, before segmentation}
%   \label{img:result_original_img}
%   \end{center}
% \end{figure}
%
% \begin{figure}
%   \begin{center}
%   \includegraphics[width=2.8in]{./../code/result/segmented_image.png}
%   \caption{Segmented image output}
%   \label{img:result_segmented_img}
%   \end{center}
% \end{figure}
%
% \begin{figure}
%   \begin{center}
%   \includegraphics[width=2.8in]{./../code/result/cost_function_convergence_segmentation.png}
%   \caption{Neural Network convergence of cost function on image color segmentation problem}
%   \label{img:result_convergence}
%   \end{center}
% \end{figure}

\section {Conclusion}

It was clear, therefore, that the Neural Network worked as expected. Both test cases (greater than function and xor function) the Neural Network worked as expected.

For the color segmentation, the Neural Network also provided a concise well-segmented result.

\vfill
\end{document}
